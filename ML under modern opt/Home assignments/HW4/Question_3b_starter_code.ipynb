{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSj1r3t62iai"
      },
      "source": [
        "While developing the code/debugging, connect to **CPU**, so that you don't use up all of your GPU resources. After making sure that the code is running, you can connect to GPU and run your full experiments there. The GPU is only used for the embeddings extraction (to speed up the code), but you can also not use it if you don't want to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8QWdGJjt81mL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-30 05:22:58.064346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1730262178.076214  954268 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1730262178.079887  954268 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-30 05:22:58.091858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tqdm import tqdm\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GOWHdTPg81mO"
      },
      "outputs": [],
      "source": [
        "# First, load the provided files to Colab in the Files section (left toolbar)\n",
        "train_set = pd.read_csv(\"data/train_set.csv\")\n",
        "test_set = pd.read_csv(\"data/test_set.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SCzUVJoy81mO"
      },
      "outputs": [],
      "source": [
        "X_train_tabular = train_set.drop(columns=[\"target\", \"text\"])\n",
        "X_test_tabular = test_set.drop(columns=[\"target\", \"text\"])\n",
        "\n",
        "y_train = train_set[\"target\"]\n",
        "y_test = test_set[\"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "faMTlIhj81mO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "{'max_depth': 3, 'n_estimators': 100}\n",
            "Accuracy: 0.562\n"
          ]
        }
      ],
      "source": [
        "### Step 1: XGBoost on Tabular Data Only ###\n",
        "\n",
        "# XGBoost on tabular data\n",
        "xgb_tabular = xgb.XGBClassifier(objective='multi:softmax', num_class=4, eta=0.1, max_depth = 5, n_estimators=100)\n",
        "\n",
        "# Fit the model\n",
        "# # TODO\n",
        "# xgb_tabular.fit(X_train_tabular, y_train)\n",
        "gridsearchxgb = GridSearchCV(xgb_tabular, {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 200]}, verbose=1)\n",
        "gridsearchxgb.fit(X_train_tabular, y_train)\n",
        "print(gridsearchxgb.best_params_)\n",
        "xgb_tabular = gridsearchxgb.best_estimator_\n",
        "xgb_tabular.fit(X_train_tabular, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "# TODO\n",
        "y_pred = xgb_tabular.predict(X_test_tabular)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IYkAxPRB81mP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Step 2: BERT Embeddings for Text Data ###\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lph9zz1481mQ"
      },
      "outputs": [],
      "source": [
        "# Function to extract BERT embeddings\n",
        "def get_bert_embeddings(texts, tokenizer, bert_model):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    # Move inputs to GPU\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    bert_model.eval()\n",
        "    outputs = bert_model(inputs['input_ids'])\n",
        "    # Use the mean of the last hidden state as the embeddings\n",
        "    return np.mean(outputs.last_hidden_state.detach().cpu().numpy(), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N7zaED-p-eal"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(df):\n",
        "    \"\"\"\n",
        "    Parameters::\n",
        "        df: DataFrame with a column named \"text\"\n",
        "\n",
        "    Returns::\n",
        "        emb_df: DataFrame with 768 columns; each row contains the embeddings for the text in the corresponding row of df.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    # Loop through the rows of the dataframe. Pass the text through the bert model and get embeddings using the get_bert_embeddings function\n",
        "    for i in tqdm(range(0, df.shape[0])):\n",
        "        text = df.iloc[i][\"text\"]\n",
        "        full_embedding = get_bert_embeddings(texts =  text,tokenizer= tokenizer, bert_model=bert_model) # TODO\n",
        "        embeddings.append(full_embedding.flatten())\n",
        "    emb_df =  pd.DataFrame(np.array(embeddings), columns=[f\"emb_{i}\" for i in range(768)])\n",
        "\n",
        "    emb_df = emb_df.set_index(df.index)\n",
        "\n",
        "    return emb_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gxx5ZnUP81mQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:39<00:00, 127.62it/s]\n",
            "100%|██████████| 1000/1000 [00:07<00:00, 128.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "{'max_depth': 3, 'n_estimators': 50}\n",
            "Accuracy with BERT embeddings: 0.561\n"
          ]
        }
      ],
      "source": [
        "# Apply BERT embeddings on train and test text data\n",
        "X_train_text_bert = create_embeddings(train_set)\n",
        "X_test_text_bert = create_embeddings(test_set)\n",
        "\n",
        "### Step 3: XGBoost on Notes Data Only ###\n",
        "\n",
        "# XGBoost on text data\n",
        "xgb_text = xgb.XGBClassifier(objective='multi:softmax', num_class=4, eta=0.1, max_depth = 5, n_estimators=100)\n",
        "\n",
        "# Fit the model\n",
        "gridsearchxgb = GridSearchCV(xgb_text, {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 200]}, verbose=1)\n",
        "gridsearchxgb.fit(X_train_text_bert, y_train)\n",
        "print(gridsearchxgb.best_params_)\n",
        "xgb_text = gridsearchxgb.best_estimator_\n",
        "xgb_text.fit(X_train_text_bert, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_bert = xgb_text.predict(X_test_text_bert)\n",
        "\n",
        "accuracy_bert = accuracy_score(y_test, y_pred_bert)\n",
        "\n",
        "print(f\"Accuracy with BERT embeddings: {accuracy_bert}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "POfMKkiWTKDB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "{'max_depth': 3, 'n_estimators': 50}\n",
            "Accuracy with combined data: 0.589\n"
          ]
        }
      ],
      "source": [
        "### Step 4: Combined Tabular and BERT Embeddings ###\n",
        "\n",
        "# Combine tabular features and BERT embeddings\n",
        "X_train_combined = pd.concat([X_train_tabular, X_train_text_bert], axis=1)\n",
        "X_test_combined = pd.concat([X_test_tabular, X_test_text_bert], axis=1)\n",
        "\n",
        "# XGBoost on combined data\n",
        "xgb_combined = xgb.XGBClassifier(objective='multi:softmax', num_class=4, eta=0.1, max_depth = 5, n_estimators=100)\n",
        "\n",
        "# Fit the model\n",
        "gridsearchxgb = GridSearchCV(xgb_combined, {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 200]}, verbose=1)\n",
        "gridsearchxgb.fit(X_train_combined, y_train)\n",
        "print(gridsearchxgb.best_params_)\n",
        "xgb_combined = gridsearchxgb.best_estimator_\n",
        "xgb_combined.fit(X_train_combined, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "accuracy_combined = accuracy_score(y_test, xgb_combined.predict(X_test_combined))\n",
        "\n",
        "print(f\"Accuracy with combined data: {accuracy_combined}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4nNYvUpYVBa"
      },
      "source": [
        "By performing a grid search, the performance of the xgboost model improves a bit compared to single modality models. That means that the two different modalities capture different information of about the data, that will be used by xgboost to have better predictions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
